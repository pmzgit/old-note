## 图灵机
* 可计算性理论
## 冯·诺依曼体系结构
* 运算器、控制器、存储器、输入设备和输出设备

## cpu
* 算术逻辑单元（Arithmetic Logic Unit，ALU）和处理器寄存器（Processor Register）的处理器单元（Processing Unit），用来完成各种算术和逻辑运算。
* 指令寄存器（Instruction Register）和程序计数器（Program Counter）的控制器单元（Control Unit/CU），用来控制程序的流程

## 思维导图
![](./assets/计算机组成原理-思维导图.jpg)

### 性能
性能指标：响应时间 + 吞吐率    
性能定义：1 / 响应时间  
计算机的计时单位：CPU 时钟,程序实际花费的 CPU 执行时间（CPU Time），就是 user time 加上 sys time。(sh: time)  
程序的 CPU 执行时间 = 指令数×CPI（Cycles Per Instruction，简称 CPI）×时钟周期（Clock Cycle Time ）

### 性能提升
* 功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量

想要计算得快，一方面，我们要在 CPU 里，同样的面积里面，多放一些晶体管，也就是增加密度，提升**制程**；另一方面，我们要让晶体管“打开”和“关闭”得更快一点，也就是提升主频。而这两者，都会增加功耗，带来**耗电和散热**的问题。

降低电压，功耗平方指数级降低

* 并行优化  
前提：符合 forkjoin 任务模型  
优化计算规律：阿姆达尔定律：优化后的执行时间 = 受优化影响的执行时间 / 加速倍数 + 不受影响的执行时间

* 加速大概率事件：向量与矩阵计算： cpu->gpu->tpu

* 通过流水线提高性能：指令分工

* 通过预测提高性能：分支和冒险、“局部性原理”

# 计算机指令和运算

## 指令集

* 高级语言->汇编语言->机器码，高级语言在转换成为机器码的时候，是通过编译器进行的，需要编译器指定编译成哪种汇编 / 机器码。编译器如果支持编译成不同的体系结构的汇编 / 机器码，就要维护很多不同的对应关系表，但是这个表并不会太大。以最复杂的 Intel X86 的指令集为例，也只有 2000 条不同的指令而已。
* 汇编代码（ASM，Assembly Language）和机器码之间是一一对应的，“汇编语言”其实可以理解成“机器码”的一种别名或者书写方式，不同的 CPU 有不同的指令集，不同的指令集和体系结构的机器会有不同的“机器码”。
* 指令集分类  
  * 数据传输类型的指令，⽐如 store/load 是寄存器与内存间数据传输的指令， mov 是将⼀个内存地
  * 址的数据移动到另⼀个内存地址的指令；  
  * 运算类型的指令，⽐如加减乘除、位运算、⽐较⼤⼩等等，它们最多只能处理两个寄存器中的数据；  
  * 跳转类型的指令，通过修改程序计数器的值来达到跳转执⾏指令的过程，⽐如编程中常⻅ifelse 、 swtich-case 、函数调⽤等。  
  * 信号类型的指令，⽐如发⽣中断的指令 trap ；  
  * 闲置类型的指令，⽐如指令 nop ，执⾏后 CPU 会空转⼀个周期；  
![](assets\mips_指令集.jpg)
## 寄存器  
逻辑上，我们可以认为，CPU 其实就是由一堆寄存器组成的。而寄存器就是 CPU 内部，由多个触发器（Flip-Flop）或者锁存器（Latches）组成的简单电路。N 个触发器或者锁存器，就可以组成一个 N 位（Bit）的寄存器，能够保存 N 位的数据。比方说，我们用的 64 位 Intel 服务器，寄存器就是 64 位的(CPU**位宽**越⼤，可以计算的数值就越⼤，⽐如说32位CPU能计算的最⼤整数是 4294967295 )。触发器和锁存器，其实就是两种不同原理的数字电路组成的逻辑门
* 线路位宽：传输分串行和并行，为了避免低效率的串⾏传输的⽅式，线路的位宽最好⼀次就能访问到所有的内存地址。想要 CPU 操作 4G 的内存，那么就需要 32 条地址总线，因为 2 ^ 32 =
4G
* 硬件的 64 位和 32 位指的是CPU的位宽，软件的 64 位和 32 位指的是指令的位宽。
### 分类与运行
* PC 寄存器：指令地址寄存器(存储 CPU 要执⾏下⼀条指令「所在的内存地址」)、指令寄存器、条件码寄存器、其他用来存储数据和内存地址的通⽤寄存器
* 一个程序执行的时候，CPU 会根据 PC 寄存器里的地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。可以看到，一个程序的一条条指令，在内存里面是连续保存的，也会一条条顺序加载。这个⾃增的⼤⼩，由CPU 的位宽决定，⽐如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会⾃增 4；

## cpu执行：指令封装与复用
* 内存栈：函数间调用，通过压栈帧（返回地址、参数列表）和出栈帧，在指令跳转的过程种，加入了一个“记忆”的功能，能够实现更加丰富和灵活的指令执行流程
* 优化：编译时内联（原因：CPU 需要执行的指令数变少了，根据地址跳转的过程不需要了，压栈和出栈的过程也不用了。缺点：程序占用空间变大），避免栈溢出（减少参数列表或者函数作用域里面的临时变量的占用内存，避免递归层数过深）

## 编译、链接和装载
* C语言代码，可以通过编译器编译成汇编代码，然后汇编代码再通过汇编器变成 CPU 可以理解的机器码也叫：目标文件（Object File）。再通过链接器（Linker）把多个目标文件以及调用的各种函数库（os相关：系统调用，动态链接库）链接起来，我们才能得到一个可执行文件。
* 第一个部分由编译（Compile）、汇编（Assemble）以及链接（Link）三个阶段组成，生成了一个可执行文件。第二部分，我们通过装载器（Loader）把可执行文件装载（Load）到内存中。CPU 从内存中读取指令和数据，来开始真正执行程序。编译器在编译程序的时候，会构造指令，这个过程叫做指令的编码。CPU 执⾏程序的时候，就会解析指令，这个过程叫作指令的解码。
* Linux 下，可执行文件和目标文件所使用的都是一种叫 ELF（Execuatable and Linkable File Format）的文件格式，中文名字叫可执行与可链接文件格式
* elf与静态链接机制   
ELF 文件格式把各种信息，分成一个一个的 Section 保存起来。ELF 有一个基本的文件头（File Header），用来表示这个文件的基本属性，比如是否是可执行文件，对应的 CPU、操作系统等等。代码段（正文段），数据段，重定位表（Relocation Table），符号表（Symbol Table）。符号表保留了我们所说的当前文件里面定义的函数名称和对应地址的地址簿。

* Windows 的可执行文件格式是一种叫作 PE（Portable Executable Format）的文件格式。
* 装载器：wine、wsl
* 高级语言：Java的类加载是由jvm完成，大致过程为装载-链接-初始化-运行，所以是jvm帮我们屏蔽了操作系统之间的差异。为了加快程序启动速度，一些类会延迟加载，所以jvm中有很多动态链接。
* 编程语言是自举的，指的是说，我们能用自己写出来的程序编译自己。比如，这里说到的 Go，先是有了 Go 语言，我们通过 C++ 写了编译器 A。然后呢，我们就可以用这个编译器 A，来编译 Go 语言的程序。接着，我们再用 Go 语言写一个编译器程序 B，然后用 A 去编译 B，就得到了 Go 语言写好的编译器的可执行文件了。这个之后，我们就可以一直用 B 来编译未来的 Go 语言程序，这也就实现了所谓的自举了。所以，即使是自举，也通常是先有了别的语言写好的编译器，然后再用自己来写自己语言的编译器。
* 编译器会把 a = 1 + 2 翻译成 4 条指令，存放到正⽂段中。如图，这 4 条指令被存放到了 0x200 ~ 0x20c的区域中
![](assets\编译.jpg)
### 程序装载
* 要求
1. 可执行程序加载后占用的内存空间应该是连续的:程序计数器是顺序地一条一条指令执行下去
2. 我们需要同时加载很多个程序，并且不能让程序自己规定在内存中加载的位置（让程序不需要考虑实际的物理内存地址、大小和当前连续分配空间问题）。
* 解决方案：加入一个间接层
1. 内存页映射：虚拟内存地址（Virtual Memory Address 指令里用到的内存地址，对于任何一个程序来说，它看到的都是同样的内存地址）与物理内存地址（Physical Memory Address实际在内存硬件里面的空间地址）映射表。
2. 内存分页： getconf PAGE_SIZE（4096）
3. 内存交换与虚拟内存：分配映射表后可以延迟加载，当cpu发出内存缺页错误（Page Fault，还没有加载或者交换出去在硬盘上），os将对应的页，从存放在硬盘上的虚拟内存里读取出来，加载到物理内存里。

### 动态链接
* 编译出来的共享库文件的指令代码，是地址无关码（Position-Independent Code）。动态代码库内部的变量和函数调用，使用相对地址（Relative Address）。
* 动态链接的解决方案：
1. 程序链接表（Procedure Link Table）里面找要调用的函数。
2. 在动态链接对应的共享库，我们在共享库的 data section 里面，保存了一张全局偏移表（GOT，Global Offset Table）。虽然共享库的代码部分的物理内存是共享的，但是数据部分是各个动态链接它的应用程序里面各加载一份的。所有需要引用当前共享库外部的地址的指令，都会查询 GOT，来找到当前运行程序的虚拟内存里的对应位置。而 GOT 表里的数据，则是在我们加载一个个共享库的时候写进去的。不同的进程，调用同样的 lib.so，各自 GOT 里面指向最终加载的动态链接库里面的虚拟内存地址是不同的。
3. PLT是为了做延迟绑定，如果函数没有实际被调用到，就不需要更新GOT里面的数值。因为很多动态装载的函数库都是不会被实际调用到的。
* **不仅能够做到代码在开发阶段的复用，也能做到代码在运行阶段的复用。**

### 二进制编码
* ASCII 码里面，数字 9 不再像整数表示法里一样，用 0000 1001 来表示，而是用 0011 1001 来表示。字符串 15 也不是用 0000 1111 这 8 位来表示，而是变成两个字符 1 和 5 连续放在一起，也就是 0011 0001 和 0011 0101，需要用两个 8 位来表示。
* 字符集（Charset）和字符编码（Character Encoding）: Unicode，其实就是一个字符集，包含了 150 种语言的 14 万个不同的字符。而字符编码则是对于字符集里的这些字符，怎么一一用二进制表示出来的一个字典

### 门电路
* 继电器（Relay）：中继，其实就是不断地通过新的电源重新放大已经开始衰减的原有信号
* 反向器（Inverter）:反向器的电路，其实就是开关从默认关闭变成默认开启而已
* 门电路，叫作组合逻辑电路。与、或、非,异或（XOR）门
### 加法器(超前进位加法器)
* 通过一个异或门计算出个位，通过一个与门计算出是否进位，我们就通过电路算出了一个一位数的加法。于是，我们把两个门电路打包，给它取一个名字，就叫作半加器（Half Adder）。
* 我们用两个半加器和一个或门，就能组合成一个全加器。
* 8 位加法器可以由 8 个全加器串联而成
* 程序知道运算溢出的原因
* https://nieyong.github.io/wiki_cpu/CPU%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84-RISC%E5%92%8CCISC.html
### 乘法器
* 是用更少更简单的电路，但是需要更长的门延迟和时钟周期；还是用更复杂的电路，但是更短的门延迟和时钟周期来计算一个复杂的指令，这之间的权衡，其实就是计算机体系结构中 RISC 和 CISC 的经典历史路线之争。

### 进制转换与表示
* 整数  
十进制转换为二进制: 除2取余，逆序排列  
二进制转换为十进制 : 按权相加法  
* 实数   
十进制转换为二进制：   
整数部分除 2 取余，逆序排列，小数部分乘 2 取整，顺序排列  
二进制转换为十进制： 按权相加法 

### 浮点数据的存储导致精度损失
* 先转成二进制，在转成IEEE754
* IEEE 754：浮点型分为符号位,指数位和有效位，x=(−1)S×(1.M)×2e，符号位 S：0是正数, 1是负数，指数位 e（指数位在这里兼具了需要表示正负的责任，它规定最大值/2 - 1表示指数0，单精度浮点数指数区域能够表示的最大数为2^8-1 = 255，那么255/2 - 1 = 127, 127代表指数0，128代表指数1, 126代表指数-1），有效位M（浮点数据的小数部分）

### 精度运算解决方案
* 转换成整型
* 使用Decimal函数
* 大数吃小数：[Kahan_summation_algorithm](https://en.wikipedia.org/wiki/Kahan_summation_algorithm)

# 处理器
## 指令+计算
### 指令周期（Instruction Cycle）
* Fetch（取得指令），也就是从 PC 寄存器里找到对应的指令地址，根据指令地址从内存里把具体的指令，加载到指令寄存器中，然后把 PC 寄存器自增，好在未来执行下一条指令,是由控制器操作的。
* Decode（指令译码），也就是根据指令寄存器里面的指令，解析成要进行什么样的操作，是 R、I、J 中的哪一种指令，具体要操作哪些寄存器、数据或者内存地址,是由控制器操作的。
* Execute（执行指令），也就是实际运行对应的 R、I、J 这些特定的指令，进行算术逻辑操作、数据传输,条件分⽀操作都是由算术逻辑单元操作的，也就是由运算器处理的。但是如果是⼀个简单的⽆条件地址跳转，则是直接在控制器⾥⾯完成的，不需要⽤到运算器。
*  CPU 将计算结果存回寄存器或者将寄存器的值存⼊内存，这个部分称为 Store（数据回写）；
![](./assets/指令周期.jpeg)
* 指令译码器将输入的机器码，解析成不同的操作码和操作数，然后传输给 ALU 进行计算
* 对于一个指令周期来说，我们取出一条指令，然后执行它，至少需要两个 CPU 周期。从内存里面读取一条指令的最短时间，称为 CPU 周期，复杂的指令则需要更多的 CPU 周期.一个指令周期，包含多个 CPU 周期，而一个 CPU 周期包含多个时钟周期,一个时钟周期里面，能够放下的是最耗时间的某一个指令步骤。
* 单看一条指令，其实一定需要很多个时钟周期。也就是说，从响应时间的角度来看，一个时钟周期一定是不够执行一条指令的。但是呢，因为有流水线，我们同时又会去执行很多个指令的不同步骤。再加上后面讲的像超线程技术等等，从吞吐量的角度来看，我们又能够做到，平均一个时钟周期里面，完成指令数可以超过 1。
### 组合逻辑电路和时序逻辑电路（自动运行，存储，时序协调）
* ALU 这样的组合逻辑电路、用来存储数据的锁存器和 D 触发器电路、用来实现 PC 寄存器的计数器电路，以及用来解码和寻址的译码器电路。
* 逻辑1和0的本质，是电压：在高电压区域的一个范围，对应数字逻辑1；在低电压区的一个范围，代表数字逻辑0，也就是所谓的高低电平。
* 反馈电路（Feedback Circuit）：把电路的输出信号作为输入信号，再回到当前电路，反相器（Inverter），非门，能够产生时钟信号。通过“外频 + 倍频“的方式来实现高频率的时钟信号
* 寄存器：通过 D 型触发器来构造,一个 D 型触发器，只能控制 1 个比特的读写，但是如果我们同时拿出多个 D 型触发器并列在一起，并且把用同一个 CLK 信号控制作为所有 D 型触发器的开关，这就变成了一个 N 位的 D 型触发器，也就可以同时控制 N 位的读写。。（当两个开关都断开的时候，最终的输出结果，取决于之前动作的输出结果，这个也就是我们说的记忆功能）。电路的输出信号不单单取决于当前的输入信号，还要取决于输出信号之前的状态。最常见的这个电路就是我们的 D 触发器，它也是我们实际在 CPU 内实现存储功能的寄存器的实现方式。
![](./assets\cpu_pc.jpg)
* 每次自增之后，我们可以去对应的 D 型触发器里面取值，这也是我们下一条需要运行指令的地址。前面第 5 讲我们讲过，同一个程序的指令应该要顺序地存放在内存里面。这里就和前面对应上了，顺序地存放指令，就是为了让我们通过程序计数器就能定时地不断执行新指令。
### cpu 运行
![](./assets\cpu_run.jpeg)
* 通过流水线、分支预测等技术，来实现在一个周期里同时执行多个指令。
* 无条件跳转并不涉及到alu的计算，因此，可以直接设置PC寄存器中的值来实现跳转

## 面向流水线的指令设计
* 提高吞吐率（流水线深度+CPU主频同时增加）：在同一时间，同时执行 5 条不同指令的不同阶段，ARM 或者 Intel 的 CPU，流水线级数都已经到了 14 级。但是，每增加一级的流水线，就要多一级写入到流水线寄存器（Pipeline Register）的操作。虽然流水线寄存器非常快，比如只有 20 皮秒（ps，10−12 秒），但深度增加，占比增加。
* 数据冒险，结构冒险、控制冒险：解决方案：乱序执行、分支预测，分级不要太多（多线程不适合有依赖关系的逻辑运算）
### 结构冒险（本质上是一个硬件层面的资源竞争问题）
* 同一个时钟周期，两个不同指令访问同一个资源：现代 CPU 架构，借鉴了哈佛架构，在高速缓存层面拆分成指令缓存和数据缓存，它会从主内存把指令和数据加载到高速缓存中，这样后续的访问都是访问高速缓存。
### 数据冒险：三种不同的依赖关系
* 先写后读（数据依赖），先读后写（反依赖），写后再写（输出依赖）。除了读之后再进行读，你会发现，对于同一个寄存器或者内存地址的操作，都有明确强制的顺序要求。而这个顺序操作的要求，也为我们使用流水线带来了很大的挑战。
* 解决：在进行指令译码的时候，会拿到对应指令所需要访问的寄存器和内存地址。所以，在这个时候，我们能够判断出来，这个指令是否会触发数据冒险（实际的CPU硬件里面有专门的冒险检测电路。）。如果会触发数据冒险，我们就可以决定，让整个流水线停顿一个或者多个周期（插入一个 NOP 操作，也就是执行一个其实什么都不干的操作，又被叫作流水线冒泡（Pipeline Bubble））。该方案是以牺牲 CPU 性能为代价的。因为，实际上在最差的情况下，我们的流水线架构的 CPU，又会退化成单指令周期的 CPU 了。delay slot是MIPS对这个的具体实现和解决方案

### 操作数前推
* 通过 NOP 操作进行对齐：有些指令没有对应的流水线阶段，但是我们并不能跳过对应的阶段直接执行下一阶段。不然，如果我们先后执行一条 LOAD 指令和一条 ADD 指令，就会发生 LOAD 指令的 WB 阶段和 ADD 指令的 WB 阶段，在同一个时钟周期发生。这样，相当于触发了一个结构冒险事件，产生了资源竞争。各个指令不需要的阶段，并不会直接跳过，而是会运行一次 NOP 操作。通过插入一个 NOP 操作，我们可以使后一条指令的每一个 Stage，一定不和前一条指令的同 Stage 在一个时钟周期执行。
* 指令对齐和流水线冒泡导致的cpu空转
![](./assets\cpu_数据冒险.jpeg)
* 操作数前推（Operand Forwarding）或操作数旁路（Operand Bypassing）
![](./assets\cpu_操作数转发.jpeg)
在 CPU 的硬件里面，需要再单独拉一根信号传输的线路出来，使得 ALU 的计算结果，能够重新回到 ALU 的输入里来。这样的一条线路，就是我们的“旁路”。它越过（Bypass）了写入寄存器，再从寄存器读出的过程，也为我们节省了 2 个时钟周期。
* 操作数前推的解决方案不但可以单独使用，还可以和流水线冒泡一起使用
![](./assets\cpu_操作数旁路.jpeg)

### 乱序执行
* 整个乱序执行技术，就好像在指令的执行阶段提供一个“线程池”。指令不再是顺序执行的，而是根据池里所拥有的资源，以及各个任务是否可以进行执行，进行动态调度。在执行完成之后，又重新把结果在一个队列里面，按照指令的分发顺序重新排序。即使内部是“乱序”的，但是在外部看起来，仍然是井井有条地顺序执行。
![](./assets\cpu_乱序执行.jpeg)
* 指令执行的先后顺序，不再和它们在程序中的顺序有关。我们只要保证不破坏数据依赖就好了。CPU 只要等到在指令结果的最终提交的阶段，再通过重排序的方式，确保指令“实际上”是顺序执行的。
* https://en.wikipedia.org/wiki/Tomasulo_algorithm
* 保障内存数据访问顺序的模型，叫作强内存模型（Strong Memory Model）
### 控制冒险（Control Harzard）:为了确保能取到正确的指令，而不得不进行等待延迟
* 在结构冒险和数据冒险中，所有的流水线停顿操作都要从指令执行阶段开始，但是条件、循环逻辑等要等 jmp 指令执行完成，去更新了 PC 寄存器之后，我们才能知道，是否执行下一条指令，还是跳转到另外一个内存地址，去取别的指令。
* 控制冒险策略，有缩短分支延迟，分支预测，动态分支预测
* 分支预测：交换内外循环的顺序，因为控制冒险导致性能差异。虽然执行的指令数是一样的，但是分支预测失败得多的程序，性能就要差上几倍。
### CPU的吞吐率超过1
* 超标量，也就是 Superscalar 这个方法。超标量可以让 CPU 不仅在指令执行阶段是并行的，在取指令和指令译码的时候，也是并行的。依赖于在硬件层面，能够检测到对应的指令的先后依赖关系，解决“冒险”问题。
* 超长指令字（VLIW）：通过在编译器层面，直接分析出指令的前后依赖关系。于是，硬件在代码编译之后，就可以直接拿到调换好先后顺序的指令。并且这些指令中，可以并行执行的部分，会打包在一起组成一个指令包。安腾处理器在取指令和指令译码的时候，拿到的不再是单个指令，而是这样一个指令包。并且在指令执行阶段，可以并行执行指令包里所有的指令。

### 线程级并行和指令集并行
* 超线程（Hyper-Threading）：在一个物理 CPU 核心内部，会有双份的 PC 寄存器、指令寄存器乃至条件码寄存器。这样，这个 CPU 核心就可以维护两条并行的指令的状态。超线程的目的，是在一个线程 A 的指令，在流水线里停顿的时候，让另外一个线程去执行指令。两个线程间没有指令依赖。适合：磁盘io和网络io比较多的情况下

* 单指令多数据流（SIMD）：单指令，一个寄存器一次性可以加载 4 个整数。比起循环分别读取 4 次对应的数据，时间就省下来了。MMX（Matrix Math eXtensions，矩阵数学扩展）、SSE、AVX，多媒体处理、机器学习算法等矩阵运算，在处理向量计算的情况下，同一个向量的不同维度之间的计算是相互独立的。可视为“数据并行”的加速方案

### 异常和中断
* 计算机会为每一种可能会发生的异常，分配一个异常代码（Exception Number）。有些教科书会把异常代码叫作中断向量（Interrupt Vector）。在捕捉异常的时候，由硬件 CPU 在进行相应的操作，而在处理异常层面，则是由作为软件的异常处理程序进行相应的操作。I/O 发出的信号的异常代码，是由操作系统来分配的，也就是由软件来设定的。而像加法溢出这样的异常代码，则是由 CPU 预先分配好的，也就是由硬件来分配的。这又是另一个软件和硬件共同组合来处理异常的过程。CPU 在拿到了异常码之后，会先把当前的程序执行的现场，保存到程序栈里面，然后根据异常码查询-中断向量表（Interrupt Vector Table），找到对应的异常处理程序，最后把后续指令执行的指挥权，交给这个异常处理程序。
* 异常的处理：上下文切换(寄存器->内核栈(用户态和内核态之间的切换)和程序栈)
![](./assets\cpu_中断.jpeg)
### 复杂指令集（Complex Instruction Set Computing，简称 CISC）和精简指令集（Reduced Instruction Set Computing，简称 RISC）
* RISC 的通过减少 CPI 来提升性能，而 CISC 通过减少需要的指令数来提升性能。
![](./assets\cpu_risc.jpeg)
RISC 的 CPU 里完成指令的电路变得简单了，于是也就腾出了更多的空间。这个空间，常常被拿来放通用寄存器。因为 RISC 完成同样的功能，执行的指令数量要比 CISC 多，所以，如果需要反复从内存里面读取指令或者数据到寄存器里来，那么很多时间就会花在访问内存上。于是，RISC 架构的 CPU 往往就有更多的通用寄存器。RISC 的 CPU 也可以把更多的晶体管，用来实现更好的分支预测等相关功能，进一步去提升 CPU 实际的执行效率。
* 微指令（Micro-Instructions/Micro-Ops）架构。在机器码层面保留了 CISC 风格的x86 架构的指令集,但是，通过指令译码器和 L0 缓存的组合，使得这些指令可以快速翻译成 RISC 风格的微指令，使得实际执行指令的流水线可以用 RISC 的架构来搭建。而微指令架构的引入，也让 CISC 和 RISC 的分界变得模糊了。
![](./assets\cpu_micro_ops.jpeg)
* ARM-Advanced RISC Machines: https://riscv.org/

### GPU
* 基于多边形建模的三维图形的渲染
顶点处理（Vertex Processing）:每一个顶点位置的转换，互相之间没有依赖，是可以并行独立计算的   
图元处理（Primitive Processing）:各个顶点连起来,变成多边形,其实转化后的顶点，仍然是在一个三维空间里，只是第三维的 Z 轴，是正对屏幕的“深度”。 针对这些多边形，需要做一个操作，叫剔除和裁剪（Cull and Clip），也就是把不在屏幕里面，或者一部分不在屏幕里面的内容给去掉，减少接下来流程的工作量。    
栅格化（Rasterization）: 转换成屏幕里面的一个个像素点。这个栅格化操作，每一个图元都可以并行独立地栅格化。  
片段处理（Fragment Processing）:计算每一个像素的颜色、透明度等信息，给像素点上色,可并行。  
像素操作（Pixel Operations）：把不同的多边形的像素点“混合（Blending）”到一起

* 统一着色器架构（Unified Shader Architecture）:因为指令集是一样的，顶点处理、图元处理、片段处理这些任务，都交给这些 Shader 去处理，让整个 GPU 尽可能地忙起来。
![](assets\GPGPU.jpeg)
* 芯片瘦身：没有 CPU 这样的分支预测和乱序执行电路.之后，基于渲染管线里面顶点处理和片段处理就是天然可以并行的了。我们在 GPU 里面可以加上很多个核。
![](./assets\gpu.jpeg)
* SIMT（Single Instruction，Multiple Threads）：SIMT 呢，比 SIMD 更加灵活。在 SIMD 里面，CPU 一次性取出了固定长度的多个数据，放到寄存器里面，用一个指令去执行。而 SIMT，可以把多条数据，交给不同的线程去处理。相同的代码和相同的流程，可能执行不同的具体的指令。在取指令和指令译码的阶段，取出的指令可以给到后面多个不同的 ALU 并行进行运算。这样，我们的一个 GPU 的核里，就可以放下更多的 ALU，同时进行更多的并行运算了。
* 超线程：因为芯片瘦身，GPU 里的指令，可能会遇到和 CPU 类似的“流水线停顿”问题,要调度一个不同的任务过来，我们就需要针对这个任务，提供更多的执行上下文。一个 Core 里面的执行上下文的数量，需要比 ALU 多

* https://www.techpowerup.com/gpu-specs/geforce-rtx-2080.c3224  
2080 一共有 46 个 SM（Streaming Multiprocessor，流式处理器），这个 SM 相当于 GPU 里面的 GPU Core，所以你可以认为这是一个 46 核的 GPU，有 46 个取指令指令译码的渲染管线。每个 SM 里面有 64 个 Cuda Core。你可以认为，这里的 Cuda Core 就是我们上面说的 ALU 的数量或者 Pixel Shader 的数量，46x64 呢一共就有 2944 个 Shader。然后，还有 184 个 TMU，TMU 就是 Texture Mapping Unit，也就是用来做纹理映射的计算单元，它也可以认为是另一种类型的 Shader。  
[Introduction-to-GPUs](./assets\Introduction-to-GPUs.pdf)

2080 的主频是 1515MHz，如果自动超频（Boost）的话，可以到 1700MHz。而 NVidia 的显卡，根据硬件架构的设计，每个时钟周期可以执行两条指令。所以，能做的浮点数运算的能力，就是：（2944 + 184）× 1700 MHz × 2  = 10.06  TFLOPS

### 现场可编程门阵列（Field-Programmable Gate Array）
* LUT（Look-Up Table，查找表）电路: 其实就是一块存储空间，里面存储了“特定的输入信号下，对应输出 0 还是 1”,代替需要的硬连线的电路，有了可编程的逻辑门
* 逻辑簇（Logic Cluster）: 对于需要实现的时序逻辑电路，我们可以在 FPGA 里面直接放上 D 触发器，作为寄存器。组合了多个 LUT 和寄存器的设备，也被叫做 CLB（Configurable Logic Block，可配置逻辑块）。
* FPGA，常常被我们用来进行芯片的设计和验证工作，也可以直接拿来当成专用的芯片，替换掉 CPU 或者 GPU，以节约成本。
### ASIC（Application-Specific Integrated Circuit），也就是专用集成电路
* 研发成本高，但制造，能耗成本低
* 用特制的硬件，最大化特定任务的运行效率
### TPU
* 做各种深度学习的推断而设计出来的，TPU 并不需要太灵活的可编程能力，只要能够迭代完成常见的深度学习推断过程中一层的计算过程就好了。所以，TPU 的硬件构造里面，把矩阵乘法、累加器和激活函数都做成了对应的专门的电路。
* 了满足深度学习推断功能的响应时间短的需求，TPU 设置了很大的使用 SRAM 的 Unified Buffer（UB），就好像一个 CPU 里面的寄存器一样，能够快速响应对于这些数据的反复读取。
* 为了让 TPU 尽可能快地部署在数据中心里面，TPU 采用了现有的 PCI-E 接口，可以和 GPU 一样直接插在主板上，并且采用了作为一个没有取指令功能的协处理器
* 因为机器学习的推断功能，通常做了数值的归一化，所以对于矩阵乘法的计算精度要求有限，整个矩阵乘法的计算模块采用了 8 Bits 来表示浮点数，而不是像 Intel CPU 里那样用上了 32 Bits（32 位浮点数的精度，差不多可以到 1/1600 万）。
### 虚拟机
* 解释型虚拟机：用应用程序解释需要模拟的计算机系统的程序格式和指令。宿主机（Host）-模拟器（Emulator）-客户机（Guest VM）
* 这种解释执行方式的最大的优势就是，模拟的系统可以跨硬件。eg:android Emulator,jvm.
* 缺点：做不到精确的“模拟”硬件。性能差。 JIT 这样的“编译优化”的办法，通过收集程序运行中反复执行的中间代码，直接编译成机器指令来执行
* 虚拟机监视器，英文叫 VMM（Virtual Machine Manager）或者 Hypervisor。Type-1型：实际的指令不需要再通过宿主机的操作系统，而可以直接通过虚拟机监视器访问硬件，KVM、XEN 还是微软自家的 Hyper-V，其实都是系统级的程序，嵌入在操作系统内核里面。需要包含能够直接操作硬件的驱动程序。所以 Type-1 的虚拟机监视器更大一些，同时兼容性也不能像 Type-2 型那么好。
* Docker 并没有再单独运行一个客户机的操作系统，而是直接运行在宿主机操作系统的内核之上只能算是一种资源隔离的技术而已。

# 存储器
## 总览
* SRAM（Static Random-Access Memory，静态随机存取存储器）：每个 CPU 核心都有一块属于自己的 L1 高速缓存，通常分成指令缓存和数据缓存，分开存放 CPU 使用的指令和数据。L1 的 Cache 往往就嵌在 CPU 核心的内部。L2 的 Cache 同样是每个 CPU 核心都有的，不过它往往不在 CPU 核心的内部。所以，L2 Cache 的访问速度会比 L1 稍微慢一些。而 L3 Cache，则通常是多个 CPU 核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。
```sh
* L1 Cache 「数据」缓存的容量⼤⼩  
cat /sys/devices/system/cpu/cpu0/cache/index0/size  
* L1 Cache 「指令」缓存的容量⼤⼩
cat /sys/devices/system/cpu/cpu0/cache/index1/size 
L2 Cache 的容量
cat /sys/devices/system/cpu/cpu0/cache/index2/size
L3 Cache 的容量
cat /sys/devices/system/cpu/cpu0/cache/index3/size
查看 CPU 的 Cache Line
cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size
```
* DRAM（Dynamic Random Access Memory，动态随机存取存储器）芯片：因为 DRAM 需要靠不断地“刷新”，才能保持数据被存储起来。DRAM 的一个比特，只需要一个晶体管和一个电容就能存储。所以，DRAM 在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大，功耗更低，有更⼤的容量，⽽且造价⽐ SRAM 芯⽚便宜很多。因为数据是存储在电容里的，电容会不断漏电，所以需要定时刷新充电，才能保持数据不丢失。内存里的数据断电后也会丢失的原因。DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问延时也就更长。
* SSD（Solid-state drive 或 Solid-state disk，固态硬盘）：
* HDD（Hard Disk Drive，硬盘）
* 各个存储器只和相邻的一层存储器打交道，并且随着一层层向下，存储器的容量逐层增大，访问速度逐层变慢，而单位存储成本也逐层下降。性能差异和价格差异至少在一个数量级。程序执⾏时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核⼼独有的 L2 Cache，最后进⼊到最快的 L1 Cache，之后CPU 再从CPU Cache 读取数据
![](./assets\storage_total.png)
[Latency Numbers Every Programmer Should Know](https://colin-scott.github.io/personal_website/research/interactive_latency.html)
## 缓存优化：局部性原理+读多写少
* 硬盘容量是内存的 16 倍乃至 128 倍，但是它们的访问速度却不到内存的 1/1000。CPU 从 L1 Cache 读取数据的速度，相⽐从内存读取的速度，会快 100 多倍。
![](./assets\storage_diff.png)
* 局部性原理（Principle of Locality）包括时间局部性（temporal locality）和空间局部性（spatial locality）这两种策略+LRU（Least Recently Used）缓存淘汰算法+缓存命中率。可以用最低的成本提供实际所需要的数据存储、管理和访问的需求。
* **估算 + 规划**：1s=1000ms=1000*1000us=1000*1000*1000ns 访问一次内存需要100ns, 那么1秒可以访问1s/100ns=10,000,000次。在实际的应用程序中，查看一个商品的数据可能意味着不止一次的随机内存或者随机磁盘的访问。对应的数据存储空间也不止要考虑数据，还需要考虑维护数据结构的空间，而缓存的命中率和访问请求也要考虑均值和峰值的问题。硬件规划：需求评估,访问的数据量以及访问的数据分布，然后根据这些数据的估算，来组合不同的存储器，能用尽可能低的成本支撑所需要的服务器压力。而当你用上了数据访问的局部性原理，组合起了多种存储器，你也就理解了怎么基于存储器层次结构，来进行硬件规划了。

## 高速缓存-读
* 一次内存的访问，大约需要 120 个 CPU Cycle，这也意味着，CPU 和内存的访问速度已经有了 120 倍的差距。按照摩尔定律，CPU 的访问速度每 18 个月便会翻一番，相当于每年增长 60%。内存的访问速度虽然也在不断增长，却远没有这么快，每年只增长 7% 左右。随着时间变迁，CPU 和内存之间的性能差距越来越大。为了弥补 CPU 与内存两者之间的性能差异，就在 CPU 内部引⼊了 CPU Cache
* 现代 CPU 中大量的空间已经被 SRAM 占据，CPU 从内存中读取数据到 CPU Cache 的过程中，是一小块一小块（Cache Line（缓存块）64 字节）来读取数据的，CPU Cache 的命中率通常能达到 95% 以上。
* 直接映射 Cache（Direct Mapped Cache）：CPU 要访问的内存数据，存储在 Cache 的哪个位置？Cache 采用 mod 的方式，把内存块映射到对应的 CPU Cache 中。实际上：把缓存块的数量设置成 2 的 N 次方，这样在计算取模的时候，可以直接取Block 地址的低 N 位，也就是二进制里面的后几位，就能得到对应的 Cache Line 地址（mod -> 位运算）。将内存中数据加载进cache中时，是一个block一个block（内存块（Block），大小和cache line一致）的读写。CPU从cache中读取数据时，是根据偏移量读取block中的一个字（64位是内存寻址空间，通常也是数据通路的字长（word size））。总结一下，一个内存的访问地址，最终包括高位代表的组标记、低位代表的索引，以及在对应的 Data Block 中定位对应字的位置偏移量。
![](./assets\l3_cache.png)
* 而内存地址对应到 Cache 里的数据结构，则多了一个有效位和对应的数据，由“索引 + 有效位  + 组标记 + 数据”组成。如果内存中的数据已经在 CPU Cache 里了，那一个内存地址的访问，就会经历这样 4 个步骤：
1. 根据内存地址的低位，计算在 Cache 中的索引；
2. 判断有效位，确认 Cache 中的数据是有效的；
3. 对比内存访问地址的高位，和 Cache 中的组标记，确认 Cache 中的数据就是我们要访问的内存数据，从 Cache Line 中读取到对应的数据块（Data Block）；
4. 根据内存地址的 Offset 位，从 Data Block 中，读取希望读取到的字。

如果在 2、3 这两个步骤中，CPU 发现，Cache 中的数据并不是要访问的内存地址的数据，那 CPU 就会访问内存，并把对应的 Block Data 更新到 Cache Line 中，同时更新对应的有效位和组标记的数据。CPU Cache 带来的毫秒乃至微秒级别的性能差异，又能带来巨大的商业利益.

* 其他通过内存
地址找到 CPU Cache 中的数据的策略，⽐如全相连 Cache （Fully Associative Cache）、组相连 Cache（Set Associative Cache）等

## 高速缓存-写
* 在 CPU Cache 里，对于数据的写入，我们也有写直达和写回这两种解决方案。在写回这个策略里，如果我们大量的操作，都能够命中缓存。那么大部分时间里，我们都不需要读写主内存，只更新缓存,只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到主内存里.
![](./assets\cache_write.jpeg)
* volatile 保障我们对于数据的读写都会到达主内存。否则，如果内存中的这个变量被别的因素（其他线程、中断函数、信号处理函数、DMA控制器、其他硬件设备）所改变了，就产生数据不一致的问题。另外，寄存器访问指令的速度要比内存访问指令的速度快，这里说的内存也包括缓存，也就是说内存访问指令实际上也有可能访问的是缓存里的数据，但即便如此，还是不如访问寄存器快的。缓存对于编译器也是透明的，编译器使用内存读写指令时只会认为是在读写内存，内存和缓存间的数据同步由CPU保证。
* https://developer.ibm.com/languages/java/

## MESI协议-缓存一致性
* 总线嗅探（Bus Snooping）机制:把所有的读写请求都通过总线（Bus）广播给所有的 CPU 核心，然后让各个核心去“嗅探”这些请求，再根据本地的情况进行响应，能够实现事务的串行化。
![](assets\l123_cache.jpeg)
* MESI 协议：基于写失效的缓存一致性协议，是支持回写（write-back）缓存的最常用协议。整个 MESI 的状态变更，则是根据来自自己 CPU 核心的请求，以及来自其他 CPU 核心通过总线传输过来的操作信号和地址信息，进行状态流转的一个有限状态机。
![wiki](assets\mesi.jpeg)
* [wiki](https://zh.wikipedia.org/wiki/MESI%E5%8D%8F%E8%AE%AE)
* [极客时间](https://time.geekbang.org/column/article/109874)

## 虚拟内存和内存保护
* 机器指令里面的内存地址都是虚拟内存地址。程序里面的每一个进程，都有一个属于自己的虚拟内存地址空间。我们可以通过地址转换来获得最终的实际物理地址。我们每一个指令都存放在内存里面，每一条数据都存放在内存里面。页号是20位，另外12位是偏移地址。 每页大小是4KB，4K=2^12B，所以要在4KB的页面里面进行寻址，偏移量是12位。 要表示4GB的内存，4GB = 2^32 = 2^20 * 2^12，所以需要2^20页，所以页号是20位。
* 在整个进程的内存地址空间，通常是“两头实、中间空”。在程序运行的时候，内存地址从顶部往下，不断分配占用的栈的空间。而堆的空间，内存地址则是从底部往上，是不断分配占用的。通常是两段连续的空间，很可能只需要很少填满的 2 级页表，甚至只需要 1 张 3 级页表就够了。多级页表就像一个多叉树的数据结构，常称为页表树（Page Table Tree）.多级页表虽然节约了我们的存储空间，却带来了时间上的开销，所以它其实是一个“以时间换空间”的策略。
* 程序所需要使用的指令，都顺序存放在虚拟内存里面。我们执行的指令，也是一条条顺序执行下去的。也就是说，我们对于指令地址的访问，存在前面几讲所说的“空间局部性”和“时间局部性”，而需要访问的数据也是一样的。我们连续执行了 5 条指令。因为内存地址都是连续的，所以这 5 条指令通常都在同一个“虚拟页”里。
* CPU 里放了一块缓存芯片。这块缓存芯片我们称之为 TLB，全称是地址变换高速缓冲（Translation-Lookaside Buffer）,在实际进行地址转换的 MMU 旁边放上了 TLB 这个用于地址转换的缓存。TLB 也像 CPU Cache 一样，分成指令和数据部分，也可以进行 L1、L2 这样的分层。
![](assets\tlb.jpeg)
* 可执行空间保护和虚拟地址空间布局随机化:通过让数据空间里面的内容不能执行，可以避免了类似于“注入攻击”的攻击方式。通过随机化虚拟内存空间的分配，可以避免让一个进程的内存里面的代码，被推测出来，从而不容易被攻击。
![](assets\random_ram.jpeg)
## 总线
* 总线（Bus）: 各个设备间的通信, 每一个设备或者功能电路模块，都要和其他 N−1 个设备去通信。为了简化系统的复杂度，我们就引入了总线，把这个 N2 的复杂度，变成一个 N 的复杂度。注册在总线上的各个模块就是松耦合的。模块互相之间并没有依赖关系。但是总线不能**同时**给多个设备提供通信功能,所以需要总线裁决（Bus Arbitraction）  
https://en.wikipedia.org/wiki/Arbiter_(electronics)  
https://github.com/google/guava/wiki/EventBusExplained  
* CPU 和内存以及高速缓存通信的总线，这里面通常有两种总线。这种方式，我们称之为双独立总线（Dual Independent Bus，缩写为 DIB）。CPU 里，有一个快速的本地总线（Local Bus），以及一个速度相对较慢的前端总线（Front-side Bus）。**速率不同**

* 我们的前端总线，其实就是系统总线。CPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I/O 桥接器（I/O Bridge）。这个 I/O 桥接器，一边接入了我们的内存总线，使得我们的 CPU 和内存通信；另一边呢，又接入了一个 I/O 总线，用来连接 I/O 设备。总线本身的电路功能，又可以拆分成用来传输数据的数据线、用来传输地址的地址线，以及用来传输控制信号的控制线，比如中断、设备复位等信号，CPU收到信号后⾃然进⾏响应。
![](assets\bus.jpeg)
* 2008 年之后，我们的 Intel CPU 其实已经没有前端总线了。Intel 发明了快速通道互联（Intel Quick Path Interconnect，简称为 QPI）技术，替代了传统的前端总线。  
https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect

## 输入输出设备
* 在 I/O 设备这一侧，我们把 I/O 设备拆分成，能和 CPU 通信的接口电路，以及实际的 I/O 设备本身。接口电路里面有对应的状态寄存器、命令寄存器、数据寄存器、数据缓冲区和设备内存等等。接口电路通过总线和 CPU 通信，接收来自 CPU 的指令和数据。而接口电路中的控制电路，再解码接收到的指令，实际去操作对应的硬件设备。
![](assets\io_device.jpeg)

* 信号和地址：发挥总线的价值.为了让已经足够复杂的 CPU 尽可能简单，计算机会把 I/O 设备的各个寄存器，以及 I/O 设备内部的内存地址，都映射到主内存地址空间里来。主内存的地址空间里，会给不同的 I/O 设备预留一段一段的内存地址。CPU 想要和这些 I/O 设备通信的时候呢，就往这些地址发送数据。这些地址信息，就是通过上一讲的地址线来发送的，而对应的数据信息呢，自然就是通过数据线来发送的了。I/O 设备呢，就会监控地址线，并且在 CPU 往自己地址发送数据的时候，把对应的数据线里面传输过来的数据，接入到对应的设备里面的寄存器和内存里面来。CPU 无论是向 I/O 设备发送命令、查询状态还是传输数据，都可以通过这样的方式。这种方式呢，叫作内存映射IO（Memory-Mapped I/O，简称 MMIO）。
* Intel X86 架构的计算机，有专门的和 I/O 设备通信的指令，也就是 in 和 out 指令。支持端口映射 I/O（Port-Mapped I/O，简称 PMIO）或者也可以叫独立输入输出（Isolated I/O）。PMIO 里面访问的设备地址，不再是在内存地址空间里面，而是一个专门的端口（Port）。通过软件层面对于传输的命令数据的定义，而不是提供特殊的新的指令，来实际操作对应的 I/O 硬件。  
https://www.coursera.org/learn/jisuanji-zucheng

## 理解IO_WAIT：I/O性能
* 顺序读写差距大：现在的 HDD 硬盘，用的是 SATA 3.0 的接口。而 SSD 硬盘呢，通常会用两种接口，一部分用的也是 SATA 3.0 的接口；另一部分呢，用的是 PCI Express 的接口。SATA 3.0 的接口，带宽是 6Gb/s相当于768MB/s,HDD 硬盘的顺序读写200MB/s左右.PCI Express，读取2GB/s左右，差不多是HDD 硬盘的10倍，写入1.2GB/s。顺序读写的响应时间：HDD几毫秒到十几毫秒，SSD几十微秒  
https://louwrentius.com/understanding-storage-performance-iops-and-latency.html
* 随机读写差距小：固态IOPS：随机读1万。随机写2万。机械IOPS：100左右，和我们 CPU 的每秒20亿次操作的能力比起来，可就差得远了
* top看是否wa（CPU 等待 IO 完成操作花费的时间占 CPU 的百分比）。iostat：实际各个硬盘读写的 tps、kB_read/s 和 kb_wrtn/s 的指标。iotop：定位到到底是哪一个进程在进行大量的 I/O 操作。  
https://github.com/ColinIanKing/stress-ng
## 机械硬盘
* 机械硬盘的硬件，主要由盘面、磁头和悬臂三部分组成。我们的数据在盘面上的位置，可以通过磁道、扇区和柱面（多个盘面上相同磁道组成的圆柱是柱面）来定位。实际的一次对于硬盘的访问，需要把盘面旋转到某一个“几何扇区”，对准悬臂的位置。然后，悬臂通过寻道，把磁头放到我们实际要读取的扇区上。
* 受制于机械硬盘的结构，我们对于随机数据的访问速度，就要包含旋转盘面的平均延时和移动悬臂的寻道时间。通过这两个时间，我们能计算出机械硬盘的 IOPS。
* 7200 转机械硬盘的 IOPS，只能做到 100 左右。在互联网时代的早期，我们也没有 SSD 硬盘可以用，所以工程师们就想出了 Partial Stroking 这个浪费存储空间，但是可以缩短寻道时间来提升硬盘的 IOPS 的解决方案。这个解决方案，也是一个典型的、在深入理解了硬件原理之后的软件优化方案。  
https://community.broadcom.com/symantecenterprise/communities/community-home/librarydocuments/viewdocument?DocumentKey=e6fb4a1b-fa13-4956-b763-8134185c0c0a&CommunityKey=63b01f30-d5eb-43c7-9232-72362b508207&tab=librarydocuments
## SSD硬盘
* SSD 的物理原理，也就是“电容 + 电压计”的组合，记录了一个或者多个比特（CPU Cache 用的 SRAM 是用一个电容来存放一个比特的数据），4 个比特一共可以从 0000-1111 表示 16 个不同的数。那么，如果我们能往电容里面充电的时候，充上 15 个不同的电压，并且我们电压计能够区分出这 15 个不同的电压。加上电容被放空代表的 0，就能够代表从 0000-1111 这样 4 个比特了。要想表示 15 个不同的电压，充电和读取的时候，对于精度的要求就会更高。这会导致充电和读取的时候都更慢，所以 QLC 的 SSD 的读写速度，要比 SLC 的慢上好几倍。
* 在控制电路里，有一个很重要的模块，叫作 FTL（Flash-Translation Layer），也就是闪存转换层。这个可以说是 SSD 硬盘的一个核心模块，SSD 硬盘性能的好坏，很大程度上也取决于 FTL 的算法好不好。SSD硬盘实际 I/O 设备的物理构造，也就是裸片（很多个裸片（Die）叠在一起的）、平面（一张裸片上可以放多个，一般一个平面上的存储容量大概在 GB 级别）、块（一个平面上面，会划分成很多个块（Block），一般一个块（Block）的存储大小， 通常几百 KB 到几 MB 大小）、页（一个块里面，还会区分很多个页（Page），就和我们内存里面的页一样，一个页的大小通常是 4KB。）的层次结构，对于数据的写入，只能是一页一页的，不能对页进行覆写。对于数据的擦除，只能整块进行。所以，我们需要用一个，类似“磁盘碎片整理”或者“内存垃圾回收”这样的机制，来清理块当中的数据空洞。而 SSD 硬盘也会保留一定的预留空间，避免出现硬盘无法写满的情况（处在最下面的两层块和页非常重要）。SSD 的使用寿命，其实是每一个块（Block）的擦除的次数。SSD 硬盘，特别适合读多写少的应用。在日常应用里面，我们的系统盘适合用 SSD。
* 在逻辑块地址和物理块地址之间，引入 FTL 这个映射层，使得操作系统无需关心物理块的擦写次数，而是由 FTL 里的软件算法，来协调到底每一次写入应该磨损哪一块。FTL 能够记录下来，每个物理块被擦写的次数。如果一个物理块被擦写的次数多了，FTL 就可以将这个物理块，挪到一个擦写次数少的物理块上。但是，逻辑块不用变
* 操作系统在删除数据的时候，并没有真的删除物理层面的数据，而只是修改了 inode 里面的数据。这个“伪删除”，使得 SSD 硬盘在逻辑和物理层面，都没有意识到有些块其实已经被删除了。这就导致在垃圾回收的时候，会浪费很多不必要的读写资源。现在的操作系统和 SSD 的主控芯片，都支持 TRIM 命令。这个命令可以在文件被删除的时候，让操作系统去通知 SSD 硬盘，对应的逻辑块已经标记成已删除了
* SSD 这个需要进行垃圾回收（trim标记+合并+擦除）的特性，使得我们在写入数据的时候，会遇到写入放大。明明我们只是写入了 4MB 的数据，可能在 SSD 的硬件层面，实际写入了 8MB、16MB 乃至更多的数据。写入放大的倍数越多，意味着实际的 SSD 性能也就越差，而解决写入放大，需要我们在后台定时进行垃圾回收，在硬盘比较空闲的时候，就把搬运数据、擦除数据、留出空白的块的工作做完，而不是等实际数据写入的时候，再进行这样的操作。  
https://zh.wikipedia.org/wiki/%E9%97%AA%E5%AD%98#NAND_Flash
## 直接内存访问（Direct Memory Access）
* DMA 技术就是我们在主板上放一块独立的芯片。在进行内存和 I/O 设备的数据传输的时候，我们不再通过 CPU 来控制数据传输，而直接通过 DMA 控制器（DMA Controller，简称 DMAC）。这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。想要主动发起数据传输，必须要是一个主设备才可以，CPU 就是主设备。而我们从设备（比如硬盘）只能接受数据传输。DMAC 它既是一个主设备，又是一个从设备。对于 CPU 来说，它是一个从设备；对于硬盘这样的 IO 设备来说呢，它又变成了一个主设备。DMAC 最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。随着现代计算机各种外设硬件越来越多，光一个通用的 DMAC 芯片不够了，我们在各个外设上都加上了 DMAC 芯片，使得 CPU 很少再需要关心数据传输的工作了。
* 设置dma：首先是源地址的初始值（数据在内存里面的地址或硬盘的 I/O 接口的地址）以及传输时候的地址增减方式。其次是目标地址初始值和传输时候的地址增减方式。第三个自然是要传输的数据长度，也就是我们一共要传输多少数据。
![](assets\dma.jpeg)
* 在 Kafka 里，通过 Java 的 NIO 里面 FileChannel 的 transferTo 方法调用，我们可以不用把数据复制到我们应用程序的内存里面。通过 DMA 的方式，我们可以把数据从内存缓冲区直接写到网卡的缓冲区里面。在使用了这样的零拷贝的方法之后呢，我们传输同样数据的时间，可以缩减为原来的 1/3，相当于提升了 3 倍的吞吐率。
![](assets\no_zero.jpg)
![](assets\zero_copy.jpg)
## 数据完整性
* https://zh.wikipedia.org/wiki/%E5%BE%AA%E7%92%B0%E5%86%97%E9%A4%98%E6%A0%A1%E9%A9%97
* 抹除码将一个讯息由n个区块变成一个讯息超过n个区块，原本的讯息可以由新的讯息的区块子集合所重建。  
[ECC纠错码-海明码](https://time.geekbang.org/column/article/123407)

## 分布式计算
* 一方面是因为垂直扩展不可持续；另一方面，则是只有水平扩展才能保障高可用性。而通过水平扩展保障高可用性，则需要我们做三件事情。第一个是理解可用性是怎么计算的。服务器硬件的损坏只是可能导致可用性损失的因素之一，机房内的电力、散热、交换机、网络线路，都有可能导致可用性损失。而外部的光缆、自然灾害，也都有可能造成我们整个系统的不可用。所以，在分析设计系统的时候，我们需要尽可能地排除单点故障。进一步地，对于硬件的故障，我们还要有自动化的健康检测和故障转移策略。SPOF
## dmp
* 因为低延时、高并发、写少读多的 DMP 的 KV 数据库，最适合用 SSD 硬盘，并且采用专门的 KV 数据库是最合适的。我们可以选择之前文章里提过的 AeroSpike，也可以用开源的 Cassandra 来提供服务。
* 对于数据管道，因为主要是顺序读和顺序写，所以我们不一定要选用 SSD 硬盘，而可以用 HDD 硬盘。不过，对于最大化吞吐量的需求，使用 zero-copy 和 DMA 是必不可少的，所以现在的数据管道的标准解决方案就是 Kafka 了。
* 对于数据仓库，我们通常是一次写入、多次读取。并且，由于存储的数据量很大，我们还要考虑成本问题。于是，一方面，我们会用 HDD 硬盘而不是 SSD 硬盘；另一方面，我们往往会预先给数据规定好 Schema，使得单条数据的序列化，不需要像存 JSON 或者 MongoDB 的 BSON 那样，存储冗余的字段名称这样的元数据。所以，最常用的解决方案是，用 Hadoop 这样的集群，采用 Hive 这样的数据仓库系统，或者采用 Avro/Thrift/ProtoBuffer 这样的二进制序列化方案。
* 从底层的存储系统的特性和原理去考虑问题.在大型的 DMP 系统设计当中，我们需要根据各个应用场景面临的实际情况，选择不同的硬件和软件的组合，来作为整个系统中的不同组件。
![](assets\dmp.jpg)
* 我们的 DMP 需要的访问场景，其实没有复杂的索引需求，但是会有比较高的并发性。我带你一看了 Facebook 开源的 Cassandra 这个分布式 KV 数据库的读写设计。通过在追加写入 Commit Log 和更新内存，Cassandra 避开了随机写的问题。内存数据的 Dump 和后台的对比合并，同样也都避开了随机写的问题，使得 Cassandra 的并发写入性能极高。在数据读取层面，通过内存缓存和 BloomFilter，Cassandra 已经尽可能地减少了需要随机读取硬盘里面数据的情况。不过挑战在于，DMP 系统的局部性不强，使得我们最终的随机读的请求还是要到硬盘上。幸运的是，SSD 硬盘在数据海量增长的那几年里价格不断下降，使得我们最终通过 SSD 硬盘解决了这个问题。

## Disruptor
* CPU 从内存加载数据到 CPU Cache 里面的时候，不是一个变量一个变量加载的，而是加载固定长度的 Cache Line。如果是加载数组里面的数据，那么 CPU 就会加载到数组里面连续的多个数据。所以，数组的遍历很容易享受到 CPU Cache 那风驰电掣的速度带来的红利。但是，多线程处理列表和数组的时候，要非常注意伪共享的问题。
* 对于类里面定义的单独的变量，就不容易享受到 CPU Cache 红利了。因为这些字段虽然在内存层面会分配到一起，但是实际应用的时候往往没有什么关联。于是，就会出现多个 CPU Core 读写的情况下，数据频繁在 CPU Cache 和内存里面来来回回的情况。而 Disruptor 很取巧地在需要频繁高速访问的变量，也就是 RingBufferFields 里面的 indexMask 这些字段前后，各定义了 7 个没有任何作用和读写请求的 long 类型的变量。这样，无论在内存的什么位置上，这些变量所在的 Cache Line 都不会有任何写更新的请求。我们就可以始终在 Cache Line 里面读到它的值，而不需要从内存里面去读取数据，也就大大加速了 Disruptor 的性能。
* 作为一个生产者 - 消费者模型，Disruptor 并没有选择使用链表来实现一个队列，而是使用了 RingBuffer。RingBuffer 底层的数据结构则是一个固定长度的数组。这个数组不仅让我们更容易用好 CPU Cache，对 CPU 执行过程中的分支预测也非常有利。更准确的分支预测，可以使得我们更好地利用好 CPU 的流水线，让代码跑得更快。
* Java 基础库里面的 BlockingQueue，都需要通过显示地加锁来保障生产者之间、消费者之间，乃至生产者和消费者之间，不会发生锁冲突的问题。但是，加锁会大大拖慢我们的性能。在获取锁过程中，CPU 没有去执行计算的相关指令，而要等待操作系统或者 JVM 来进行锁竞争的裁决。而那些没有拿到锁而被挂起等待的线程，则需要进行上下文切换。这个上下文切换，会把挂起线程的寄存器里的数据放到线程的程序栈里面去。这也意味着，加载到高速缓存里面的数据也失效了，程序就变得更慢了。
* Disruptor 里的 RingBuffer 采用了一个无锁的解决方案，通过 CAS 这样的操作，去进行序号的自增和对比，使得 CPU 不需要获取操作系统的锁。而是能够继续顺序地执行 CPU 指令。没有上下文切换、没有操作系统锁，自然程序就跑得快了。不过因为采用了 CAS 这样的忙等待（Busy-Wait）的方式，会使得我们的 CPU 始终满负荷运转，消耗更多的电，算是一个小小的缺点。

## 要想写出让 CPU 跑得更快的代码，就需要写出缓存命中率⾼的代码，CPU L1 Cache 分为数据缓存和指令缓存，因⽽需要分别提⾼它们的缓存命中率：
* 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根
据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；
* 对于指令缓存，有规律的条件分⽀语句能够让 CPU 的分⽀预测器发挥作⽤，进⼀步提⾼执⾏的效
率；
* 另外，对于多核 CPU 系统，线程可能在不同 CPU 核⼼来回切换，这样各个核⼼的缓存命中率就会受到影响，于是要想提⾼进程的缓存命中率，可以考虑把线程绑定 CPU 到某⼀个 CPU 核⼼。